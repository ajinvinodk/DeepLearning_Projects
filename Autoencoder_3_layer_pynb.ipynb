{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoder_3_layer.pynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8CYE5oJbF-N"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix"
      ],
      "metadata": {
        "id": "GF5b-n2ibU4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.load('/content/drive/MyDrive/Colab Notebooks/CS671/Assignment_2/X_train.npy')\n",
        "y_train = np.load('/content/drive/MyDrive/Colab Notebooks/CS671/Assignment_2/y_train.npy')\n",
        "X_test = np.load('/content/drive/MyDrive/Colab Notebooks/CS671/Assignment_2/X_test.npy')\n",
        "y_test = np.load('/content/drive/MyDrive/Colab Notebooks/CS671/Assignment_2/y_test.npy')\n",
        "X_validation = np.load('/content/drive/MyDrive/Colab Notebooks/CS671/Assignment_2/X_validation.npy')\n",
        "y_validation = np.load('/content/drive/MyDrive/Colab Notebooks/CS671/Assignment_2/y_validation.npy')"
      ],
      "metadata": {
        "id": "8CiHIs4nbXlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_2_4_6_7_8 = [X_train[0], X_train[2277], X_train[4554], X_train[6831], X_train[9108]]\n",
        "X_validation_2_4_6_7_8 = [X_validation[0], X_validation[759], X_validation[1518], X_validation[2277], X_validation[3036]]"
      ],
      "metadata": {
        "id": "bc9xFqOjbZ3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(2,3,1),plt.imshow(X_train[0].reshape(28,28))\n",
        "plt.subplot(2,3,2),plt.imshow(X_train[2277].reshape(28,28))\n",
        "plt.subplot(2,3,3),plt.imshow(X_train[4554].reshape(28,28))\n",
        "plt.subplot(2,3,4),plt.imshow(X_train[6831].reshape(28,28))\n",
        "plt.subplot(2,3,5),plt.imshow(X_train[9108].reshape(28,28))"
      ],
      "metadata": {
        "id": "8CN7bRG7bZ_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy = list(zip(X_train, y_train))\n",
        "random.shuffle(dummy)\n",
        "X_train, y_train = zip(*dummy)\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "dummy = list(zip(X_test, y_test))\n",
        "random.shuffle(dummy)\n",
        "X_test, y_test = zip(*dummy)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "dummy = list(zip(X_validation, y_validation))\n",
        "random.shuffle(dummy)\n",
        "X_validation, y_validation = zip(*dummy)\n",
        "X_validation = np.array(X_validation)\n",
        "y_validation = np.array(y_validation)"
      ],
      "metadata": {
        "id": "IAevGOezbaFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train1=X_train/255\n",
        "X_validation1=X_validation/255\n",
        "X_test1=X_test/255"
      ],
      "metadata": {
        "id": "-pUdFvSTbaKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(256, activation='sigmoid', name='hidden_layer_1', input_shape=(784,)),\n",
        "    tf.keras.layers.Dense(16, activation='sigmoid', name='hidden_layer_2_bottleneck'),\n",
        "    tf.keras.layers.Dense(256, activation='sigmoid', name='hidden_layer_3'),\n",
        "    tf.keras.layers.Dense(784, activation='sigmoid')\n",
        "])\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    min_delta=0.0001,\n",
        "    patience=50,\n",
        "    verbose=0,\n",
        "    mode=\"min\",\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "checkpoint_filepath = '/tmp/checkpoint'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,beta_1=0.9,beta_2=0.999,epsilon=1e-08),\n",
        "    loss=tf.keras.losses.MeanSquaredError())\n",
        "\n",
        "history = model.fit(X_train1, X_train1, epochs=1000, batch_size=32, verbose=1,\n",
        "          validation_data = (X_validation1, X_validation1),\n",
        "          callbacks=[callback, model_checkpoint_callback])\n",
        "\n",
        "model.load_weights(checkpoint_filepath)"
      ],
      "metadata": {
        "id": "QB40kbYNbaPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(history.history.keys())"
      ],
      "metadata": {
        "id": "8sRF2wm0baUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.savefig(\"model_loss_encoder.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j7JO3n-6baYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "QNL8ibnMbacV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.predict(np.array(X_train_2_4_6_7_8))\n",
        "figure(figsize=(12,9),dpi=80)\n",
        "plt.subplot(2,5,1),plt.imshow(X_train_2_4_6_7_8[0].reshape(28,28))\n",
        "plt.subplot(2,5,2),plt.imshow(X_train_2_4_6_7_8[1].reshape(28,28))\n",
        "plt.subplot(2,5,3),plt.imshow(X_train_2_4_6_7_8[2].reshape(28,28))\n",
        "plt.subplot(2,5,4),plt.imshow(X_train_2_4_6_7_8[3].reshape(28,28))\n",
        "plt.subplot(2,5,5),plt.imshow(X_train_2_4_6_7_8[4].reshape(28,28))\n",
        "plt.subplot(2,5,6),plt.imshow(pred[0].reshape(28,28))\n",
        "plt.subplot(2,5,7),plt.imshow(pred[1].reshape(28,28))\n",
        "plt.subplot(2,5,8),plt.imshow(pred[2].reshape(28,28))\n",
        "plt.subplot(2,5,9),plt.imshow(pred[3].reshape(28,28))\n",
        "plt.subplot(2,5,10),plt.imshow(pred[4].reshape(28,28))\n",
        "plt.suptitle('Reconstructed images from training dataset')\n",
        "plt.savefig(\"Reconstructed_images_from_training_dataset\")"
      ],
      "metadata": {
        "id": "9_2kuib6bagU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.predict(np.array(X_validation_2_4_6_7_8))\n",
        "figure(figsize=(12,9),dpi=80)\n",
        "plt.subplot(2,5,1),plt.imshow(X_validation_2_4_6_7_8[0].reshape(28,28))\n",
        "plt.subplot(2,5,2),plt.imshow(X_validation_2_4_6_7_8[1].reshape(28,28))\n",
        "plt.subplot(2,5,3),plt.imshow(X_validation_2_4_6_7_8[2].reshape(28,28))\n",
        "plt.subplot(2,5,4),plt.imshow(X_validation_2_4_6_7_8[3].reshape(28,28))\n",
        "plt.subplot(2,5,5),plt.imshow(X_validation_2_4_6_7_8[4].reshape(28,28))\n",
        "plt.subplot(2,5,6),plt.imshow(pred[0].reshape(28,28))\n",
        "plt.subplot(2,5,7),plt.imshow(pred[1].reshape(28,28))\n",
        "plt.subplot(2,5,8),plt.imshow(pred[2].reshape(28,28))\n",
        "plt.subplot(2,5,9),plt.imshow(pred[3].reshape(28,28))\n",
        "plt.subplot(2,5,10),plt.imshow(pred[4].reshape(28,28))\n",
        "plt.suptitle('Reconstructed images from validation dataset')\n",
        "plt.savefig(\"Reconstructed_images_from_validation_dataset\")"
      ],
      "metadata": {
        "id": "imfZMYqGbak-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Average reconstruction error\n",
        "pred = model.predict(X_train1)\n",
        "dummy = []\n",
        "for i in range(len(pred)):\n",
        "    dummy.append(pred[i]- X_train1[i])\n",
        "dummy = np.array(dummy)\n",
        "dummy1 = dummy.flatten()\n",
        "for i in range(len(dummy1)):\n",
        "    dummy1[i] = dummy1[i]*dummy1[i]\n",
        "reconstruction_error_train= np.sum(dummy1)/len(dummy1)\n",
        "print(\"Final Average reconstruction error for training data is:\", reconstruction_error_train)\n",
        "\n",
        "pred = model.predict(X_validation1)\n",
        "dummy = []\n",
        "for i in range(len(pred)):\n",
        "    dummy.append(pred[i]- X_validation1[i])\n",
        "dummy = np.array(dummy)\n",
        "dummy1 = dummy.flatten()\n",
        "for i in range(len(dummy1)):\n",
        "    dummy1[i] = dummy1[i]*dummy1[i]\n",
        "reconstruction_error_validation= np.sum(dummy1)/len(dummy1)\n",
        "print(\"Final Average reconstruction error for validation data is:\", reconstruction_error_validation)\n",
        "\n",
        "pred = model.predict(X_test1)\n",
        "dummy = []\n",
        "for i in range(len(pred)):\n",
        "    dummy.append(pred[i]- X_test1[i])\n",
        "dummy = np.array(dummy)\n",
        "dummy1 = dummy.flatten()\n",
        "for i in range(len(dummy1)):\n",
        "    dummy1[i] = dummy1[i]*dummy1[i]\n",
        "reconstruction_error_test= np.sum(dummy1)/len(dummy1)\n",
        "print(\"Final Average reconstruction error for testing data is:\", reconstruction_error_test)"
      ],
      "metadata": {
        "id": "48rVYaqTx4Vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = tf.keras.models.Sequential()\n",
        "encoder.add(model.get_layer('hidden_layer_1'))\n",
        "encoder.add(model.get_layer('hidden_layer_2_bottleneck'))"
      ],
      "metadata": {
        "id": "uBBZLD3SyBwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_encoder=encoder.predict(X_train1)\n",
        "X_validation_encoder=encoder.predict(X_validation1)"
      ],
      "metadata": {
        "id": "Y09CB4X_yGJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Classifier_model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation='sigmoid', name='hidden_layer_1', input_shape=(16,)),\n",
        "    tf.keras.layers.Dense(16, activation='sigmoid', name='hidden_layer_2'),\n",
        "    tf.keras.layers.Dense(8, activation='sigmoid', name='hidden_layer_3'),\n",
        "    tf.keras.layers.Dense(5, activation='softmax')\n",
        "])\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    min_delta=0.0001,\n",
        "    patience=50,\n",
        "    verbose=0,\n",
        "    mode=\"min\",\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "checkpoint_filepath = '/tmp/checkpoint'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "Classifier_model.compile(\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(),  \n",
        "    metrics = ['accuracy'] \n",
        ")\n",
        "\n",
        "history = Classifier_model.fit(X_train_encoder, y_train, epochs=5000, batch_size=32, verbose=1,   \n",
        "          validation_data = (X_validation_encoder, y_validation),\n",
        "          callbacks=[callback, model_checkpoint_callback]\n",
        ")\n",
        "\n",
        "Classifier_model.load_weights(checkpoint_filepath)"
      ],
      "metadata": {
        "id": "UPjjnHwWyGNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(history.history.keys())"
      ],
      "metadata": {
        "id": "-s6PB3MKyGST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.savefig(\"model_accuracy.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0RxeAkRRyGWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.savefig(\"model_loss.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5kU356pqyGaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Classifier_model.summary()"
      ],
      "metadata": {
        "id": "1J3gFyxgyGeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_encoder=encoder(X_test1)\n",
        "pred = Classifier_model.predict(X_test_encoder)\n",
        "y_pred_test = []\n",
        "for dummy in pred:\n",
        "    dummy1 = (np.argmax(dummy, axis=0))\n",
        "    y_pred_test.append(dummy1)\n",
        "\n",
        "y_test_1 = []\n",
        "for dummy in y_test:\n",
        "    dummy1 = (np.argmax(dummy, axis=0))\n",
        "    y_test_1.append(dummy1)\n",
        "\n",
        "y_pred_test = np.array(y_pred_test)\n",
        "\n",
        "cf_matrix_test = confusion_matrix(y_test_1, y_pred_test)\n",
        "print(cf_matrix_test)\n",
        "\n",
        "x_axis_labels = [2,4,6,7,8]\n",
        "y_axis_labels = [2,4,6,7,8]\n",
        "cf_matrix_test_plot = sns.heatmap(cf_matrix_test, xticklabels=x_axis_labels, yticklabels=y_axis_labels, annot=True, fmt='g')   # fmt='g': general format, annot:If True, write the data value in each cell.\n",
        "fig = cf_matrix_test_plot.get_figure()\n",
        "fig.savefig(\"confusion_matrix_test.png\") "
      ],
      "metadata": {
        "id": "P8AfVkScyGhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_encoder=encoder(X_train1)\n",
        "pred = Classifier_model.predict(X_train_encoder)\n",
        "y_pred_train = []\n",
        "\n",
        "for dummy in pred:\n",
        "    dummy1 = (np.argmax(dummy, axis=0))\n",
        "    y_pred_train.append(dummy1)\n",
        "\n",
        "y_train_1 = []\n",
        "for dummy in y_train:\n",
        "    dummy1 = (np.argmax(dummy, axis=0))\n",
        "    y_train_1.append(dummy1)\n",
        "\n",
        "y_pred_train = np.array(y_pred_train)\n",
        "\n",
        "cf_matrix_train = confusion_matrix(y_train_1, y_pred_train)\n",
        "print(cf_matrix_train)\n",
        "\n",
        "x_axis_labels = [2,4,6,7,8]\n",
        "y_axis_labels = [2,4,6,7,8]\n",
        "cf_matrix_train_plot = sns.heatmap(cf_matrix_train, xticklabels=x_axis_labels, yticklabels=y_axis_labels, annot=True, fmt='g')    # fmt='g': general format, annot:If True, write the data value in each cell.\n",
        "fig = cf_matrix_train_plot.get_figure()\n",
        "fig.savefig(\"confusion_matrix_train.png\")"
      ],
      "metadata": {
        "id": "bSRGljV41dx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_validation_encoder=encoder(X_validation1)\n",
        "pred = Classifier_model.predict(X_validation_encoder)\n",
        "y_pred_validation = []\n",
        "for dummy in pred:\n",
        "    dummy1 = (np.argmax(dummy, axis=0))\n",
        "    y_pred_validation.append(dummy1)\n",
        "\n",
        "y_validation_1 = []\n",
        "for dummy in y_validation:\n",
        "    dummy1 = (np.argmax(dummy, axis=0))\n",
        "    y_validation_1.append(dummy1)\n",
        "\n",
        "y_pred_validation = np.array(y_pred_validation)\n",
        "\n",
        "cf_matrix_validation = confusion_matrix(y_validation_1, y_pred_validation)\n",
        "print(cf_matrix_validation)\n",
        "\n",
        "x_axis_labels = [2,4,6,7,8]\n",
        "y_axis_labels = [2,4,6,7,8]\n",
        "cf_matrix_validation_plot = sns.heatmap(cf_matrix_validation, xticklabels=x_axis_labels, yticklabels=y_axis_labels, annot=True, fmt='g')   # fmt='g': general format, annot:If True, write the data value in each cell.\n",
        "fig = cf_matrix_validation_plot.get_figure()\n",
        "fig.savefig(\"confusion_matrix_validation.png\") "
      ],
      "metadata": {
        "id": "yuwVRxmB1dr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = Classifier_model.evaluate(X_test_encoder, y_test, verbose=2)\n",
        "print('Test Accuracy: %.3f' % acc)"
      ],
      "metadata": {
        "id": "6rWHCv621dih"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}